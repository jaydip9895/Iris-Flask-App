{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
       "       'species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop('species',axis=1)\n",
    "y = iris['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaydip/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jaydip/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jaydip/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jaydip/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jaydip/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jaydip/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jaydip/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=4,activation='relu',input_shape=[4,]))\n",
    "\n",
    "# Last layer for multi-class classification of 3 species\n",
    "model.add(Dense(units=3,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "WARNING:tensorflow:From /Users/jaydip/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/300\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 1.0714 - acc: 0.3333 - val_loss: 1.0978 - val_acc: 0.3333\n",
      "Epoch 2/300\n",
      "120/120 [==============================] - 0s 99us/sample - loss: 1.0677 - acc: 0.3333 - val_loss: 1.0937 - val_acc: 0.3333\n",
      "Epoch 3/300\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 1.0641 - acc: 0.3333 - val_loss: 1.0899 - val_acc: 0.3333\n",
      "Epoch 4/300\n",
      "120/120 [==============================] - 0s 306us/sample - loss: 1.0608 - acc: 0.3333 - val_loss: 1.0862 - val_acc: 0.3333\n",
      "Epoch 5/300\n",
      "120/120 [==============================] - 0s 238us/sample - loss: 1.0574 - acc: 0.3333 - val_loss: 1.0825 - val_acc: 0.3333\n",
      "Epoch 6/300\n",
      "120/120 [==============================] - 0s 283us/sample - loss: 1.0541 - acc: 0.3333 - val_loss: 1.0787 - val_acc: 0.3333\n",
      "Epoch 7/300\n",
      "120/120 [==============================] - 0s 178us/sample - loss: 1.0508 - acc: 0.3333 - val_loss: 1.0749 - val_acc: 0.3333\n",
      "Epoch 8/300\n",
      "120/120 [==============================] - 0s 155us/sample - loss: 1.0475 - acc: 0.3333 - val_loss: 1.0713 - val_acc: 0.3333\n",
      "Epoch 9/300\n",
      "120/120 [==============================] - 0s 180us/sample - loss: 1.0444 - acc: 0.3333 - val_loss: 1.0675 - val_acc: 0.3333\n",
      "Epoch 10/300\n",
      "120/120 [==============================] - 0s 231us/sample - loss: 1.0412 - acc: 0.3333 - val_loss: 1.0639 - val_acc: 0.3333\n",
      "Epoch 11/300\n",
      "120/120 [==============================] - 0s 281us/sample - loss: 1.0379 - acc: 0.3500 - val_loss: 1.0603 - val_acc: 0.3667\n",
      "Epoch 12/300\n",
      "120/120 [==============================] - 0s 165us/sample - loss: 1.0347 - acc: 0.3833 - val_loss: 1.0570 - val_acc: 0.5000\n",
      "Epoch 13/300\n",
      "120/120 [==============================] - 0s 177us/sample - loss: 1.0317 - acc: 0.4917 - val_loss: 1.0535 - val_acc: 0.6000\n",
      "Epoch 14/300\n",
      "120/120 [==============================] - 0s 163us/sample - loss: 1.0285 - acc: 0.6333 - val_loss: 1.0501 - val_acc: 0.6000\n",
      "Epoch 15/300\n",
      "120/120 [==============================] - 0s 214us/sample - loss: 1.0254 - acc: 0.6833 - val_loss: 1.0467 - val_acc: 0.6000\n",
      "Epoch 16/300\n",
      "120/120 [==============================] - 0s 154us/sample - loss: 1.0223 - acc: 0.6833 - val_loss: 1.0434 - val_acc: 0.6000\n",
      "Epoch 17/300\n",
      "120/120 [==============================] - 0s 163us/sample - loss: 1.0194 - acc: 0.6833 - val_loss: 1.0399 - val_acc: 0.6000\n",
      "Epoch 18/300\n",
      "120/120 [==============================] - 0s 182us/sample - loss: 1.0163 - acc: 0.6833 - val_loss: 1.0367 - val_acc: 0.6000\n",
      "Epoch 19/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 1.0134 - acc: 0.6833 - val_loss: 1.0334 - val_acc: 0.6000\n",
      "Epoch 20/300\n",
      "120/120 [==============================] - 0s 164us/sample - loss: 1.0105 - acc: 0.6833 - val_loss: 1.0300 - val_acc: 0.6000\n",
      "Epoch 21/300\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 1.0074 - acc: 0.6833 - val_loss: 1.0270 - val_acc: 0.6000\n",
      "Epoch 22/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 1.0048 - acc: 0.6833 - val_loss: 1.0238 - val_acc: 0.6000\n",
      "Epoch 23/300\n",
      "120/120 [==============================] - 0s 271us/sample - loss: 1.0020 - acc: 0.6833 - val_loss: 1.0210 - val_acc: 0.6000\n",
      "Epoch 24/300\n",
      "120/120 [==============================] - 0s 317us/sample - loss: 0.9994 - acc: 0.6833 - val_loss: 1.0181 - val_acc: 0.6000\n",
      "Epoch 25/300\n",
      "120/120 [==============================] - 0s 185us/sample - loss: 0.9969 - acc: 0.6833 - val_loss: 1.0153 - val_acc: 0.6000\n",
      "Epoch 26/300\n",
      "120/120 [==============================] - 0s 185us/sample - loss: 0.9944 - acc: 0.6833 - val_loss: 1.0126 - val_acc: 0.6000\n",
      "Epoch 27/300\n",
      "120/120 [==============================] - 0s 203us/sample - loss: 0.9920 - acc: 0.6833 - val_loss: 1.0098 - val_acc: 0.6000\n",
      "Epoch 28/300\n",
      "120/120 [==============================] - 0s 194us/sample - loss: 0.9896 - acc: 0.6833 - val_loss: 1.0071 - val_acc: 0.6000\n",
      "Epoch 29/300\n",
      "120/120 [==============================] - 0s 188us/sample - loss: 0.9871 - acc: 0.6833 - val_loss: 1.0046 - val_acc: 0.6000\n",
      "Epoch 30/300\n",
      "120/120 [==============================] - 0s 220us/sample - loss: 0.9848 - acc: 0.6833 - val_loss: 1.0020 - val_acc: 0.6000\n",
      "Epoch 31/300\n",
      "120/120 [==============================] - 0s 203us/sample - loss: 0.9823 - acc: 0.6833 - val_loss: 0.9996 - val_acc: 0.6000\n",
      "Epoch 32/300\n",
      "120/120 [==============================] - 0s 189us/sample - loss: 0.9799 - acc: 0.6833 - val_loss: 0.9970 - val_acc: 0.6000\n",
      "Epoch 33/300\n",
      "120/120 [==============================] - 0s 281us/sample - loss: 0.9775 - acc: 0.6833 - val_loss: 0.9945 - val_acc: 0.6000\n",
      "Epoch 34/300\n",
      "120/120 [==============================] - 0s 190us/sample - loss: 0.9751 - acc: 0.6833 - val_loss: 0.9921 - val_acc: 0.6000\n",
      "Epoch 35/300\n",
      "120/120 [==============================] - 0s 238us/sample - loss: 0.9727 - acc: 0.6917 - val_loss: 0.9895 - val_acc: 0.6000\n",
      "Epoch 36/300\n",
      "120/120 [==============================] - 0s 197us/sample - loss: 0.9702 - acc: 0.6917 - val_loss: 0.9870 - val_acc: 0.6000\n",
      "Epoch 37/300\n",
      "120/120 [==============================] - 0s 182us/sample - loss: 0.9679 - acc: 0.6917 - val_loss: 0.9844 - val_acc: 0.6000\n",
      "Epoch 38/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 0.9655 - acc: 0.6917 - val_loss: 0.9819 - val_acc: 0.6000\n",
      "Epoch 39/300\n",
      "120/120 [==============================] - 0s 201us/sample - loss: 0.9631 - acc: 0.6917 - val_loss: 0.9795 - val_acc: 0.6000\n",
      "Epoch 40/300\n",
      "120/120 [==============================] - 0s 317us/sample - loss: 0.9607 - acc: 0.6917 - val_loss: 0.9770 - val_acc: 0.6000\n",
      "Epoch 41/300\n",
      "120/120 [==============================] - 0s 213us/sample - loss: 0.9584 - acc: 0.6917 - val_loss: 0.9744 - val_acc: 0.6000\n",
      "Epoch 42/300\n",
      "120/120 [==============================] - 0s 229us/sample - loss: 0.9560 - acc: 0.6917 - val_loss: 0.9720 - val_acc: 0.6000\n",
      "Epoch 43/300\n",
      "120/120 [==============================] - 0s 202us/sample - loss: 0.9536 - acc: 0.6917 - val_loss: 0.9695 - val_acc: 0.6000\n",
      "Epoch 44/300\n",
      "120/120 [==============================] - 0s 139us/sample - loss: 0.9512 - acc: 0.6917 - val_loss: 0.9672 - val_acc: 0.6000\n",
      "Epoch 45/300\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.9489 - acc: 0.6917 - val_loss: 0.9649 - val_acc: 0.6000\n",
      "Epoch 46/300\n",
      "120/120 [==============================] - 0s 162us/sample - loss: 0.9464 - acc: 0.6917 - val_loss: 0.9625 - val_acc: 0.6000\n",
      "Epoch 47/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.9441 - acc: 0.7000 - val_loss: 0.9603 - val_acc: 0.6000\n",
      "Epoch 48/300\n",
      "120/120 [==============================] - 0s 119us/sample - loss: 0.9416 - acc: 0.7000 - val_loss: 0.9580 - val_acc: 0.6000\n",
      "Epoch 49/300\n",
      "120/120 [==============================] - 0s 120us/sample - loss: 0.9393 - acc: 0.7000 - val_loss: 0.9556 - val_acc: 0.6000\n",
      "Epoch 50/300\n",
      "120/120 [==============================] - 0s 146us/sample - loss: 0.9369 - acc: 0.7000 - val_loss: 0.9532 - val_acc: 0.6000\n",
      "Epoch 51/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.9345 - acc: 0.7000 - val_loss: 0.9508 - val_acc: 0.6000\n",
      "Epoch 52/300\n",
      "120/120 [==============================] - 0s 166us/sample - loss: 0.9320 - acc: 0.7000 - val_loss: 0.9484 - val_acc: 0.6000\n",
      "Epoch 53/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.9297 - acc: 0.7000 - val_loss: 0.9460 - val_acc: 0.6000\n",
      "Epoch 54/300\n",
      "120/120 [==============================] - 0s 177us/sample - loss: 0.9273 - acc: 0.7000 - val_loss: 0.9436 - val_acc: 0.6000\n",
      "Epoch 55/300\n",
      "120/120 [==============================] - 0s 114us/sample - loss: 0.9250 - acc: 0.7000 - val_loss: 0.9412 - val_acc: 0.6333\n",
      "Epoch 56/300\n",
      "120/120 [==============================] - 0s 159us/sample - loss: 0.9225 - acc: 0.7000 - val_loss: 0.9389 - val_acc: 0.6333\n",
      "Epoch 57/300\n",
      "120/120 [==============================] - 0s 131us/sample - loss: 0.9200 - acc: 0.7000 - val_loss: 0.9366 - val_acc: 0.6333\n",
      "Epoch 58/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.9176 - acc: 0.7000 - val_loss: 0.9343 - val_acc: 0.6333\n",
      "Epoch 59/300\n",
      "120/120 [==============================] - 0s 144us/sample - loss: 0.9152 - acc: 0.7000 - val_loss: 0.9321 - val_acc: 0.6667\n",
      "Epoch 60/300\n",
      "120/120 [==============================] - 0s 181us/sample - loss: 0.9129 - acc: 0.7000 - val_loss: 0.9297 - val_acc: 0.6667\n",
      "Epoch 61/300\n",
      "120/120 [==============================] - 0s 122us/sample - loss: 0.9103 - acc: 0.7000 - val_loss: 0.9274 - val_acc: 0.6667\n",
      "Epoch 62/300\n",
      "120/120 [==============================] - 0s 186us/sample - loss: 0.9079 - acc: 0.7000 - val_loss: 0.9250 - val_acc: 0.6667\n",
      "Epoch 63/300\n",
      "120/120 [==============================] - 0s 134us/sample - loss: 0.9054 - acc: 0.7000 - val_loss: 0.9227 - val_acc: 0.6667\n",
      "Epoch 64/300\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.9030 - acc: 0.7000 - val_loss: 0.9205 - val_acc: 0.6667\n",
      "Epoch 65/300\n",
      "120/120 [==============================] - 0s 134us/sample - loss: 0.9006 - acc: 0.7000 - val_loss: 0.9183 - val_acc: 0.6667\n",
      "Epoch 66/300\n",
      "120/120 [==============================] - 0s 134us/sample - loss: 0.8981 - acc: 0.7000 - val_loss: 0.9160 - val_acc: 0.6667\n",
      "Epoch 67/300\n",
      "120/120 [==============================] - 0s 139us/sample - loss: 0.8958 - acc: 0.7000 - val_loss: 0.9138 - val_acc: 0.6667\n",
      "Epoch 68/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.8931 - acc: 0.7000 - val_loss: 0.9116 - val_acc: 0.6667\n",
      "Epoch 69/300\n",
      "120/120 [==============================] - 0s 138us/sample - loss: 0.8907 - acc: 0.7083 - val_loss: 0.9091 - val_acc: 0.6667\n",
      "Epoch 70/300\n",
      "120/120 [==============================] - 0s 172us/sample - loss: 0.8882 - acc: 0.7083 - val_loss: 0.9069 - val_acc: 0.6667\n",
      "Epoch 71/300\n",
      "120/120 [==============================] - 0s 187us/sample - loss: 0.8858 - acc: 0.7083 - val_loss: 0.9046 - val_acc: 0.6667\n",
      "Epoch 72/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.8833 - acc: 0.7083 - val_loss: 0.9024 - val_acc: 0.6667\n",
      "Epoch 73/300\n",
      "120/120 [==============================] - 0s 145us/sample - loss: 0.8807 - acc: 0.7083 - val_loss: 0.9002 - val_acc: 0.6667\n",
      "Epoch 74/300\n",
      "120/120 [==============================] - 0s 154us/sample - loss: 0.8784 - acc: 0.7083 - val_loss: 0.8980 - val_acc: 0.6667\n",
      "Epoch 75/300\n",
      "120/120 [==============================] - 0s 226us/sample - loss: 0.8758 - acc: 0.7083 - val_loss: 0.8958 - val_acc: 0.6667\n",
      "Epoch 76/300\n",
      "120/120 [==============================] - 0s 193us/sample - loss: 0.8733 - acc: 0.7083 - val_loss: 0.8935 - val_acc: 0.6667\n",
      "Epoch 77/300\n",
      "120/120 [==============================] - 0s 188us/sample - loss: 0.8708 - acc: 0.7083 - val_loss: 0.8913 - val_acc: 0.6667\n",
      "Epoch 78/300\n",
      "120/120 [==============================] - 0s 193us/sample - loss: 0.8683 - acc: 0.7083 - val_loss: 0.8889 - val_acc: 0.7000\n",
      "Epoch 79/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.8659 - acc: 0.7167 - val_loss: 0.8867 - val_acc: 0.7000\n",
      "Epoch 80/300\n",
      "120/120 [==============================] - 0s 169us/sample - loss: 0.8633 - acc: 0.7250 - val_loss: 0.8843 - val_acc: 0.7000\n",
      "Epoch 81/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8609 - acc: 0.7250 - val_loss: 0.8822 - val_acc: 0.7000\n",
      "Epoch 82/300\n",
      "120/120 [==============================] - 0s 138us/sample - loss: 0.8583 - acc: 0.7250 - val_loss: 0.8799 - val_acc: 0.7000\n",
      "Epoch 83/300\n",
      "120/120 [==============================] - 0s 160us/sample - loss: 0.8558 - acc: 0.7250 - val_loss: 0.8777 - val_acc: 0.7000\n",
      "Epoch 84/300\n",
      "120/120 [==============================] - 0s 147us/sample - loss: 0.8533 - acc: 0.7250 - val_loss: 0.8756 - val_acc: 0.7000\n",
      "Epoch 85/300\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.8509 - acc: 0.7250 - val_loss: 0.8734 - val_acc: 0.7000\n",
      "Epoch 86/300\n",
      "120/120 [==============================] - 0s 115us/sample - loss: 0.8484 - acc: 0.7250 - val_loss: 0.8711 - val_acc: 0.7000\n",
      "Epoch 87/300\n",
      "120/120 [==============================] - 0s 169us/sample - loss: 0.8458 - acc: 0.7333 - val_loss: 0.8687 - val_acc: 0.7000\n",
      "Epoch 88/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.8434 - acc: 0.7333 - val_loss: 0.8665 - val_acc: 0.7000\n",
      "Epoch 89/300\n",
      "120/120 [==============================] - 0s 168us/sample - loss: 0.8408 - acc: 0.7333 - val_loss: 0.8643 - val_acc: 0.7000\n",
      "Epoch 90/300\n",
      "120/120 [==============================] - 0s 120us/sample - loss: 0.8383 - acc: 0.7333 - val_loss: 0.8620 - val_acc: 0.7000\n",
      "Epoch 91/300\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.8358 - acc: 0.7333 - val_loss: 0.8597 - val_acc: 0.7000\n",
      "Epoch 92/300\n",
      "120/120 [==============================] - 0s 147us/sample - loss: 0.8334 - acc: 0.7333 - val_loss: 0.8575 - val_acc: 0.7000\n",
      "Epoch 93/300\n",
      "120/120 [==============================] - 0s 194us/sample - loss: 0.8308 - acc: 0.7333 - val_loss: 0.8552 - val_acc: 0.7000\n",
      "Epoch 94/300\n",
      "120/120 [==============================] - 0s 147us/sample - loss: 0.8283 - acc: 0.7333 - val_loss: 0.8529 - val_acc: 0.7000\n",
      "Epoch 95/300\n",
      "120/120 [==============================] - 0s 164us/sample - loss: 0.8259 - acc: 0.7333 - val_loss: 0.8507 - val_acc: 0.7000\n",
      "Epoch 96/300\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.8234 - acc: 0.7333 - val_loss: 0.8487 - val_acc: 0.7000\n",
      "Epoch 97/300\n",
      "120/120 [==============================] - 0s 121us/sample - loss: 0.8208 - acc: 0.7333 - val_loss: 0.8463 - val_acc: 0.7000\n",
      "Epoch 98/300\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.8451 - acc: 0.687 - 0s 128us/sample - loss: 0.8183 - acc: 0.7333 - val_loss: 0.8441 - val_acc: 0.7000\n",
      "Epoch 99/300\n",
      "120/120 [==============================] - 0s 153us/sample - loss: 0.8159 - acc: 0.7333 - val_loss: 0.8420 - val_acc: 0.7000\n",
      "Epoch 100/300\n",
      "120/120 [==============================] - 0s 148us/sample - loss: 0.8133 - acc: 0.7333 - val_loss: 0.8398 - val_acc: 0.7000\n",
      "Epoch 101/300\n",
      "120/120 [==============================] - 0s 170us/sample - loss: 0.8107 - acc: 0.7333 - val_loss: 0.8376 - val_acc: 0.7000\n",
      "Epoch 102/300\n",
      "120/120 [==============================] - 0s 110us/sample - loss: 0.8084 - acc: 0.7333 - val_loss: 0.8357 - val_acc: 0.7000\n",
      "Epoch 103/300\n",
      "120/120 [==============================] - 0s 168us/sample - loss: 0.8058 - acc: 0.7333 - val_loss: 0.8334 - val_acc: 0.7000\n",
      "Epoch 104/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.8032 - acc: 0.7333 - val_loss: 0.8312 - val_acc: 0.7000\n",
      "Epoch 105/300\n",
      "120/120 [==============================] - 0s 166us/sample - loss: 0.8008 - acc: 0.7333 - val_loss: 0.8292 - val_acc: 0.7000\n",
      "Epoch 106/300\n",
      "120/120 [==============================] - 0s 117us/sample - loss: 0.7982 - acc: 0.7333 - val_loss: 0.8271 - val_acc: 0.7000\n",
      "Epoch 107/300\n",
      "120/120 [==============================] - 0s 120us/sample - loss: 0.7958 - acc: 0.7333 - val_loss: 0.8251 - val_acc: 0.7000\n",
      "Epoch 108/300\n",
      "120/120 [==============================] - 0s 102us/sample - loss: 0.7932 - acc: 0.7333 - val_loss: 0.8229 - val_acc: 0.7000\n",
      "Epoch 109/300\n",
      "120/120 [==============================] - 0s 111us/sample - loss: 0.7907 - acc: 0.7333 - val_loss: 0.8207 - val_acc: 0.7000\n",
      "Epoch 110/300\n",
      "120/120 [==============================] - 0s 111us/sample - loss: 0.7882 - acc: 0.7333 - val_loss: 0.8186 - val_acc: 0.7000\n",
      "Epoch 111/300\n",
      "120/120 [==============================] - 0s 106us/sample - loss: 0.7858 - acc: 0.7333 - val_loss: 0.8165 - val_acc: 0.7000\n",
      "Epoch 112/300\n",
      "120/120 [==============================] - 0s 103us/sample - loss: 0.7832 - acc: 0.7333 - val_loss: 0.8144 - val_acc: 0.7000\n",
      "Epoch 113/300\n",
      "120/120 [==============================] - 0s 104us/sample - loss: 0.7806 - acc: 0.7250 - val_loss: 0.8123 - val_acc: 0.6667\n",
      "Epoch 114/300\n",
      "120/120 [==============================] - 0s 113us/sample - loss: 0.7782 - acc: 0.7250 - val_loss: 0.8104 - val_acc: 0.6667\n",
      "Epoch 115/300\n",
      "120/120 [==============================] - 0s 132us/sample - loss: 0.7757 - acc: 0.7167 - val_loss: 0.8083 - val_acc: 0.6667\n",
      "Epoch 116/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.7733 - acc: 0.7167 - val_loss: 0.8062 - val_acc: 0.6667\n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 169us/sample - loss: 0.7707 - acc: 0.7167 - val_loss: 0.8040 - val_acc: 0.6667\n",
      "Epoch 118/300\n",
      "120/120 [==============================] - 0s 178us/sample - loss: 0.7682 - acc: 0.7167 - val_loss: 0.8018 - val_acc: 0.6667\n",
      "Epoch 119/300\n",
      "120/120 [==============================] - 0s 111us/sample - loss: 0.7657 - acc: 0.7250 - val_loss: 0.7994 - val_acc: 0.6667\n",
      "Epoch 120/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.7632 - acc: 0.7250 - val_loss: 0.7971 - val_acc: 0.6667\n",
      "Epoch 121/300\n",
      "120/120 [==============================] - 0s 160us/sample - loss: 0.7607 - acc: 0.7250 - val_loss: 0.7949 - val_acc: 0.6667\n",
      "Epoch 122/300\n",
      "120/120 [==============================] - 0s 162us/sample - loss: 0.7583 - acc: 0.7250 - val_loss: 0.7928 - val_acc: 0.6667\n",
      "Epoch 123/300\n",
      "120/120 [==============================] - 0s 182us/sample - loss: 0.7558 - acc: 0.7250 - val_loss: 0.7907 - val_acc: 0.6667\n",
      "Epoch 124/300\n",
      "120/120 [==============================] - 0s 134us/sample - loss: 0.7532 - acc: 0.7250 - val_loss: 0.7885 - val_acc: 0.6667\n",
      "Epoch 125/300\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 0.7508 - acc: 0.7250 - val_loss: 0.7864 - val_acc: 0.6667\n",
      "Epoch 126/300\n",
      "120/120 [==============================] - 0s 164us/sample - loss: 0.7483 - acc: 0.7250 - val_loss: 0.7844 - val_acc: 0.6333\n",
      "Epoch 127/300\n",
      "120/120 [==============================] - 0s 157us/sample - loss: 0.7458 - acc: 0.7250 - val_loss: 0.7823 - val_acc: 0.6333\n",
      "Epoch 128/300\n",
      "120/120 [==============================] - 0s 189us/sample - loss: 0.7433 - acc: 0.7250 - val_loss: 0.7803 - val_acc: 0.6333\n",
      "Epoch 129/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.7409 - acc: 0.7250 - val_loss: 0.7782 - val_acc: 0.6333\n",
      "Epoch 130/300\n",
      "120/120 [==============================] - 0s 154us/sample - loss: 0.7383 - acc: 0.7250 - val_loss: 0.7761 - val_acc: 0.6333\n",
      "Epoch 131/300\n",
      "120/120 [==============================] - 0s 153us/sample - loss: 0.7358 - acc: 0.7250 - val_loss: 0.7740 - val_acc: 0.6333\n",
      "Epoch 132/300\n",
      "120/120 [==============================] - 0s 147us/sample - loss: 0.7334 - acc: 0.7250 - val_loss: 0.7719 - val_acc: 0.6333\n",
      "Epoch 133/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 0.7311 - acc: 0.7250 - val_loss: 0.7700 - val_acc: 0.6333\n",
      "Epoch 134/300\n",
      "120/120 [==============================] - 0s 177us/sample - loss: 0.7284 - acc: 0.7250 - val_loss: 0.7678 - val_acc: 0.6333\n",
      "Epoch 135/300\n",
      "120/120 [==============================] - 0s 182us/sample - loss: 0.7260 - acc: 0.7250 - val_loss: 0.7655 - val_acc: 0.6333\n",
      "Epoch 136/300\n",
      "120/120 [==============================] - 0s 206us/sample - loss: 0.7235 - acc: 0.7250 - val_loss: 0.7633 - val_acc: 0.6333\n",
      "Epoch 137/300\n",
      "120/120 [==============================] - 0s 159us/sample - loss: 0.7212 - acc: 0.7250 - val_loss: 0.7613 - val_acc: 0.6333\n",
      "Epoch 138/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.7186 - acc: 0.7250 - val_loss: 0.7591 - val_acc: 0.6333\n",
      "Epoch 139/300\n",
      "120/120 [==============================] - 0s 182us/sample - loss: 0.7163 - acc: 0.7250 - val_loss: 0.7572 - val_acc: 0.6333\n",
      "Epoch 140/300\n",
      "120/120 [==============================] - 0s 171us/sample - loss: 0.7137 - acc: 0.7250 - val_loss: 0.7550 - val_acc: 0.6333\n",
      "Epoch 141/300\n",
      "120/120 [==============================] - 0s 190us/sample - loss: 0.7113 - acc: 0.7250 - val_loss: 0.7530 - val_acc: 0.6333\n",
      "Epoch 142/300\n",
      "120/120 [==============================] - 0s 179us/sample - loss: 0.7089 - acc: 0.7250 - val_loss: 0.7509 - val_acc: 0.6333\n",
      "Epoch 143/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.7064 - acc: 0.7250 - val_loss: 0.7488 - val_acc: 0.6333\n",
      "Epoch 144/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.7040 - acc: 0.7250 - val_loss: 0.7467 - val_acc: 0.6333\n",
      "Epoch 145/300\n",
      "120/120 [==============================] - 0s 244us/sample - loss: 0.7016 - acc: 0.7250 - val_loss: 0.7446 - val_acc: 0.6333\n",
      "Epoch 146/300\n",
      "120/120 [==============================] - 0s 325us/sample - loss: 0.6992 - acc: 0.7250 - val_loss: 0.7425 - val_acc: 0.6333\n",
      "Epoch 147/300\n",
      "120/120 [==============================] - 0s 194us/sample - loss: 0.6969 - acc: 0.7250 - val_loss: 0.7405 - val_acc: 0.6333\n",
      "Epoch 148/300\n",
      "120/120 [==============================] - 0s 249us/sample - loss: 0.6944 - acc: 0.7250 - val_loss: 0.7385 - val_acc: 0.6333\n",
      "Epoch 149/300\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.6921 - acc: 0.7250 - val_loss: 0.7365 - val_acc: 0.6333\n",
      "Epoch 150/300\n",
      "120/120 [==============================] - 0s 189us/sample - loss: 0.6896 - acc: 0.7250 - val_loss: 0.7345 - val_acc: 0.6333\n",
      "Epoch 151/300\n",
      "120/120 [==============================] - 0s 166us/sample - loss: 0.6872 - acc: 0.7250 - val_loss: 0.7324 - val_acc: 0.6333\n",
      "Epoch 152/300\n",
      "120/120 [==============================] - 0s 157us/sample - loss: 0.6849 - acc: 0.7250 - val_loss: 0.7304 - val_acc: 0.6333\n",
      "Epoch 153/300\n",
      "120/120 [==============================] - 0s 202us/sample - loss: 0.6825 - acc: 0.7250 - val_loss: 0.7282 - val_acc: 0.6333\n",
      "Epoch 154/300\n",
      "120/120 [==============================] - 0s 194us/sample - loss: 0.6802 - acc: 0.7250 - val_loss: 0.7261 - val_acc: 0.6333\n",
      "Epoch 155/300\n",
      "120/120 [==============================] - 0s 184us/sample - loss: 0.6778 - acc: 0.7250 - val_loss: 0.7240 - val_acc: 0.6333\n",
      "Epoch 156/300\n",
      "120/120 [==============================] - 0s 229us/sample - loss: 0.6755 - acc: 0.7250 - val_loss: 0.7221 - val_acc: 0.6333\n",
      "Epoch 157/300\n",
      "120/120 [==============================] - 0s 168us/sample - loss: 0.6731 - acc: 0.7250 - val_loss: 0.7200 - val_acc: 0.6333\n",
      "Epoch 158/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6710 - acc: 0.7250 - val_loss: 0.7182 - val_acc: 0.6333\n",
      "Epoch 159/300\n",
      "120/120 [==============================] - 0s 212us/sample - loss: 0.6685 - acc: 0.7250 - val_loss: 0.7160 - val_acc: 0.6333\n",
      "Epoch 160/300\n",
      "120/120 [==============================] - 0s 227us/sample - loss: 0.6661 - acc: 0.7250 - val_loss: 0.7138 - val_acc: 0.6333\n",
      "Epoch 161/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.6639 - acc: 0.7250 - val_loss: 0.7116 - val_acc: 0.6333\n",
      "Epoch 162/300\n",
      "120/120 [==============================] - 0s 189us/sample - loss: 0.6616 - acc: 0.7250 - val_loss: 0.7095 - val_acc: 0.6333\n",
      "Epoch 163/300\n",
      "120/120 [==============================] - 0s 213us/sample - loss: 0.6593 - acc: 0.7250 - val_loss: 0.7075 - val_acc: 0.6333\n",
      "Epoch 164/300\n",
      "120/120 [==============================] - 0s 225us/sample - loss: 0.6571 - acc: 0.7250 - val_loss: 0.7055 - val_acc: 0.6333\n",
      "Epoch 165/300\n",
      "120/120 [==============================] - 0s 193us/sample - loss: 0.6548 - acc: 0.7250 - val_loss: 0.7034 - val_acc: 0.6333\n",
      "Epoch 166/300\n",
      "120/120 [==============================] - 0s 258us/sample - loss: 0.6525 - acc: 0.7250 - val_loss: 0.7013 - val_acc: 0.6333\n",
      "Epoch 167/300\n",
      "120/120 [==============================] - 0s 225us/sample - loss: 0.6503 - acc: 0.7250 - val_loss: 0.6993 - val_acc: 0.6333\n",
      "Epoch 168/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.6481 - acc: 0.7250 - val_loss: 0.6974 - val_acc: 0.6333\n",
      "Epoch 169/300\n",
      "120/120 [==============================] - 0s 211us/sample - loss: 0.6458 - acc: 0.7250 - val_loss: 0.6953 - val_acc: 0.6333\n",
      "Epoch 170/300\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.6437 - acc: 0.7250 - val_loss: 0.6931 - val_acc: 0.6667\n",
      "Epoch 171/300\n",
      "120/120 [==============================] - 0s 162us/sample - loss: 0.6414 - acc: 0.7250 - val_loss: 0.6912 - val_acc: 0.6667\n",
      "Epoch 172/300\n",
      "120/120 [==============================] - 0s 165us/sample - loss: 0.6393 - acc: 0.7250 - val_loss: 0.6891 - val_acc: 0.6667\n",
      "Epoch 173/300\n",
      "120/120 [==============================] - 0s 136us/sample - loss: 0.6371 - acc: 0.7250 - val_loss: 0.6874 - val_acc: 0.6667\n",
      "Epoch 174/300\n",
      "120/120 [==============================] - 0s 244us/sample - loss: 0.6350 - acc: 0.7250 - val_loss: 0.6856 - val_acc: 0.6667\n",
      "Epoch 175/300\n",
      "120/120 [==============================] - 0s 155us/sample - loss: 0.6328 - acc: 0.7250 - val_loss: 0.6836 - val_acc: 0.6667\n",
      "Epoch 176/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 188us/sample - loss: 0.6306 - acc: 0.7250 - val_loss: 0.6816 - val_acc: 0.6667\n",
      "Epoch 177/300\n",
      "120/120 [==============================] - 0s 165us/sample - loss: 0.6285 - acc: 0.7250 - val_loss: 0.6796 - val_acc: 0.6667\n",
      "Epoch 178/300\n",
      "120/120 [==============================] - 0s 138us/sample - loss: 0.6264 - acc: 0.7250 - val_loss: 0.6776 - val_acc: 0.6667\n",
      "Epoch 179/300\n",
      "120/120 [==============================] - 0s 132us/sample - loss: 0.6243 - acc: 0.7250 - val_loss: 0.6758 - val_acc: 0.6667\n",
      "Epoch 180/300\n",
      "120/120 [==============================] - 0s 156us/sample - loss: 0.6222 - acc: 0.7250 - val_loss: 0.6737 - val_acc: 0.6667\n",
      "Epoch 181/300\n",
      "120/120 [==============================] - 0s 149us/sample - loss: 0.6201 - acc: 0.7250 - val_loss: 0.6717 - val_acc: 0.6667\n",
      "Epoch 182/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.6181 - acc: 0.7250 - val_loss: 0.6697 - val_acc: 0.6667\n",
      "Epoch 183/300\n",
      "120/120 [==============================] - 0s 153us/sample - loss: 0.6160 - acc: 0.7250 - val_loss: 0.6679 - val_acc: 0.6667\n",
      "Epoch 184/300\n",
      "120/120 [==============================] - 0s 196us/sample - loss: 0.6140 - acc: 0.7250 - val_loss: 0.6663 - val_acc: 0.6667\n",
      "Epoch 185/300\n",
      "120/120 [==============================] - 0s 170us/sample - loss: 0.6119 - acc: 0.7250 - val_loss: 0.6644 - val_acc: 0.6667\n",
      "Epoch 186/300\n",
      "120/120 [==============================] - 0s 156us/sample - loss: 0.6098 - acc: 0.7250 - val_loss: 0.6625 - val_acc: 0.6667\n",
      "Epoch 187/300\n",
      "120/120 [==============================] - 0s 201us/sample - loss: 0.6079 - acc: 0.7250 - val_loss: 0.6607 - val_acc: 0.6667\n",
      "Epoch 188/300\n",
      "120/120 [==============================] - 0s 177us/sample - loss: 0.6059 - acc: 0.7250 - val_loss: 0.6589 - val_acc: 0.6667\n",
      "Epoch 189/300\n",
      "120/120 [==============================] - 0s 154us/sample - loss: 0.6039 - acc: 0.7250 - val_loss: 0.6571 - val_acc: 0.6667\n",
      "Epoch 190/300\n",
      "120/120 [==============================] - 0s 145us/sample - loss: 0.6020 - acc: 0.7333 - val_loss: 0.6550 - val_acc: 0.6667\n",
      "Epoch 191/300\n",
      "120/120 [==============================] - 0s 204us/sample - loss: 0.5999 - acc: 0.7333 - val_loss: 0.6532 - val_acc: 0.6667\n",
      "Epoch 192/300\n",
      "120/120 [==============================] - 0s 243us/sample - loss: 0.5980 - acc: 0.7333 - val_loss: 0.6515 - val_acc: 0.6667\n",
      "Epoch 193/300\n",
      "120/120 [==============================] - 0s 165us/sample - loss: 0.5961 - acc: 0.7333 - val_loss: 0.6497 - val_acc: 0.7000\n",
      "Epoch 194/300\n",
      "120/120 [==============================] - 0s 193us/sample - loss: 0.5941 - acc: 0.7333 - val_loss: 0.6476 - val_acc: 0.7000\n",
      "Epoch 195/300\n",
      "120/120 [==============================] - 0s 177us/sample - loss: 0.5922 - acc: 0.7333 - val_loss: 0.6456 - val_acc: 0.7000\n",
      "Epoch 196/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.5903 - acc: 0.7333 - val_loss: 0.6437 - val_acc: 0.7000\n",
      "Epoch 197/300\n",
      "120/120 [==============================] - 0s 168us/sample - loss: 0.5885 - acc: 0.7333 - val_loss: 0.6418 - val_acc: 0.7000\n",
      "Epoch 198/300\n",
      "120/120 [==============================] - 0s 122us/sample - loss: 0.5866 - acc: 0.7333 - val_loss: 0.6400 - val_acc: 0.7000\n",
      "Epoch 199/300\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.5847 - acc: 0.7333 - val_loss: 0.6383 - val_acc: 0.7000\n",
      "Epoch 200/300\n",
      "120/120 [==============================] - 0s 99us/sample - loss: 0.5829 - acc: 0.7333 - val_loss: 0.6366 - val_acc: 0.7000\n",
      "Epoch 201/300\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.5810 - acc: 0.7333 - val_loss: 0.6348 - val_acc: 0.7000\n",
      "Epoch 202/300\n",
      "120/120 [==============================] - 0s 101us/sample - loss: 0.5792 - acc: 0.7333 - val_loss: 0.6331 - val_acc: 0.7000\n",
      "Epoch 203/300\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.5773 - acc: 0.7333 - val_loss: 0.6315 - val_acc: 0.7000\n",
      "Epoch 204/300\n",
      "120/120 [==============================] - 0s 97us/sample - loss: 0.5756 - acc: 0.7333 - val_loss: 0.6299 - val_acc: 0.7000\n",
      "Epoch 205/300\n",
      "120/120 [==============================] - 0s 111us/sample - loss: 0.5738 - acc: 0.7333 - val_loss: 0.6282 - val_acc: 0.7000\n",
      "Epoch 206/300\n",
      "120/120 [==============================] - 0s 105us/sample - loss: 0.5720 - acc: 0.7333 - val_loss: 0.6266 - val_acc: 0.7000\n",
      "Epoch 207/300\n",
      "120/120 [==============================] - 0s 98us/sample - loss: 0.5702 - acc: 0.7333 - val_loss: 0.6248 - val_acc: 0.7000\n",
      "Epoch 208/300\n",
      "120/120 [==============================] - 0s 121us/sample - loss: 0.5685 - acc: 0.7333 - val_loss: 0.6231 - val_acc: 0.7000\n",
      "Epoch 209/300\n",
      "120/120 [==============================] - 0s 154us/sample - loss: 0.5667 - acc: 0.7333 - val_loss: 0.6214 - val_acc: 0.7000\n",
      "Epoch 210/300\n",
      "120/120 [==============================] - 0s 139us/sample - loss: 0.5650 - acc: 0.7333 - val_loss: 0.6197 - val_acc: 0.7000\n",
      "Epoch 211/300\n",
      "120/120 [==============================] - 0s 176us/sample - loss: 0.5633 - acc: 0.7333 - val_loss: 0.6181 - val_acc: 0.7000\n",
      "Epoch 212/300\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 0.5616 - acc: 0.7333 - val_loss: 0.6163 - val_acc: 0.7000\n",
      "Epoch 213/300\n",
      "120/120 [==============================] - 0s 103us/sample - loss: 0.5599 - acc: 0.7417 - val_loss: 0.6146 - val_acc: 0.7000\n",
      "Epoch 214/300\n",
      "120/120 [==============================] - 0s 102us/sample - loss: 0.5582 - acc: 0.7417 - val_loss: 0.6131 - val_acc: 0.7000\n",
      "Epoch 215/300\n",
      "120/120 [==============================] - 0s 115us/sample - loss: 0.5565 - acc: 0.7417 - val_loss: 0.6114 - val_acc: 0.7000\n",
      "Epoch 216/300\n",
      "120/120 [==============================] - 0s 102us/sample - loss: 0.5549 - acc: 0.7417 - val_loss: 0.6095 - val_acc: 0.7000\n",
      "Epoch 217/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.5532 - acc: 0.7417 - val_loss: 0.6078 - val_acc: 0.7000\n",
      "Epoch 218/300\n",
      "120/120 [==============================] - 0s 115us/sample - loss: 0.5515 - acc: 0.7417 - val_loss: 0.6062 - val_acc: 0.7000\n",
      "Epoch 219/300\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.5499 - acc: 0.7417 - val_loss: 0.6046 - val_acc: 0.7000\n",
      "Epoch 220/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.5484 - acc: 0.7417 - val_loss: 0.6032 - val_acc: 0.7000\n",
      "Epoch 221/300\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.5468 - acc: 0.7417 - val_loss: 0.6013 - val_acc: 0.7000\n",
      "Epoch 222/300\n",
      "120/120 [==============================] - 0s 103us/sample - loss: 0.5451 - acc: 0.7417 - val_loss: 0.5997 - val_acc: 0.7000\n",
      "Epoch 223/300\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.5435 - acc: 0.7417 - val_loss: 0.5980 - val_acc: 0.7000\n",
      "Epoch 224/300\n",
      "120/120 [==============================] - 0s 105us/sample - loss: 0.5420 - acc: 0.7417 - val_loss: 0.5965 - val_acc: 0.7000\n",
      "Epoch 225/300\n",
      "120/120 [==============================] - 0s 70us/sample - loss: 0.5404 - acc: 0.7417 - val_loss: 0.5949 - val_acc: 0.7000\n",
      "Epoch 226/300\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.5390 - acc: 0.7417 - val_loss: 0.5931 - val_acc: 0.7000\n",
      "Epoch 227/300\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.5373 - acc: 0.7417 - val_loss: 0.5915 - val_acc: 0.7000\n",
      "Epoch 228/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5358 - acc: 0.7417 - val_loss: 0.5900 - val_acc: 0.7000\n",
      "Epoch 229/300\n",
      "120/120 [==============================] - 0s 189us/sample - loss: 0.5343 - acc: 0.7417 - val_loss: 0.5885 - val_acc: 0.7000\n",
      "Epoch 230/300\n",
      "120/120 [==============================] - 0s 271us/sample - loss: 0.5328 - acc: 0.7417 - val_loss: 0.5870 - val_acc: 0.7000\n",
      "Epoch 231/300\n",
      "120/120 [==============================] - 0s 327us/sample - loss: 0.5313 - acc: 0.7417 - val_loss: 0.5856 - val_acc: 0.7000\n",
      "Epoch 232/300\n",
      "120/120 [==============================] - 0s 159us/sample - loss: 0.5298 - acc: 0.7417 - val_loss: 0.5842 - val_acc: 0.7000\n",
      "Epoch 233/300\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.5284 - acc: 0.7417 - val_loss: 0.5827 - val_acc: 0.7000\n",
      "Epoch 234/300\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.5269 - acc: 0.7417 - val_loss: 0.5813 - val_acc: 0.7000\n",
      "Epoch 235/300\n",
      "120/120 [==============================] - 0s 128us/sample - loss: 0.5255 - acc: 0.7417 - val_loss: 0.5797 - val_acc: 0.7000\n",
      "Epoch 236/300\n",
      "120/120 [==============================] - 0s 230us/sample - loss: 0.5241 - acc: 0.7417 - val_loss: 0.5782 - val_acc: 0.7000\n",
      "Epoch 237/300\n",
      "120/120 [==============================] - 0s 117us/sample - loss: 0.5226 - acc: 0.7417 - val_loss: 0.5767 - val_acc: 0.7000\n",
      "Epoch 238/300\n",
      "120/120 [==============================] - 0s 168us/sample - loss: 0.5212 - acc: 0.7417 - val_loss: 0.5753 - val_acc: 0.7000\n",
      "Epoch 239/300\n",
      "120/120 [==============================] - 0s 154us/sample - loss: 0.5198 - acc: 0.7417 - val_loss: 0.5737 - val_acc: 0.7000\n",
      "Epoch 240/300\n",
      "120/120 [==============================] - 0s 185us/sample - loss: 0.5184 - acc: 0.7500 - val_loss: 0.5722 - val_acc: 0.7000\n",
      "Epoch 241/300\n",
      "120/120 [==============================] - 0s 202us/sample - loss: 0.5171 - acc: 0.7500 - val_loss: 0.5708 - val_acc: 0.7000\n",
      "Epoch 242/300\n",
      "120/120 [==============================] - 0s 157us/sample - loss: 0.5157 - acc: 0.7500 - val_loss: 0.5693 - val_acc: 0.7000\n",
      "Epoch 243/300\n",
      "120/120 [==============================] - 0s 184us/sample - loss: 0.5143 - acc: 0.7500 - val_loss: 0.5679 - val_acc: 0.7000\n",
      "Epoch 244/300\n",
      "120/120 [==============================] - 0s 118us/sample - loss: 0.5130 - acc: 0.7500 - val_loss: 0.5665 - val_acc: 0.7000\n",
      "Epoch 245/300\n",
      "120/120 [==============================] - 0s 146us/sample - loss: 0.5117 - acc: 0.7500 - val_loss: 0.5650 - val_acc: 0.7000\n",
      "Epoch 246/300\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.5103 - acc: 0.7500 - val_loss: 0.5637 - val_acc: 0.7000\n",
      "Epoch 247/300\n",
      "120/120 [==============================] - 0s 143us/sample - loss: 0.5090 - acc: 0.7667 - val_loss: 0.5624 - val_acc: 0.7000\n",
      "Epoch 248/300\n",
      "120/120 [==============================] - 0s 152us/sample - loss: 0.5077 - acc: 0.7667 - val_loss: 0.5612 - val_acc: 0.7000\n",
      "Epoch 249/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5065 - acc: 0.7667 - val_loss: 0.5597 - val_acc: 0.7000\n",
      "Epoch 250/300\n",
      "120/120 [==============================] - 0s 166us/sample - loss: 0.5051 - acc: 0.7667 - val_loss: 0.5584 - val_acc: 0.7000\n",
      "Epoch 251/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.5038 - acc: 0.7667 - val_loss: 0.5571 - val_acc: 0.7000\n",
      "Epoch 252/300\n",
      "120/120 [==============================] - 0s 164us/sample - loss: 0.5025 - acc: 0.7667 - val_loss: 0.5557 - val_acc: 0.7000\n",
      "Epoch 253/300\n",
      "120/120 [==============================] - 0s 157us/sample - loss: 0.5013 - acc: 0.7750 - val_loss: 0.5543 - val_acc: 0.7000\n",
      "Epoch 254/300\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.5001 - acc: 0.7750 - val_loss: 0.5531 - val_acc: 0.7000\n",
      "Epoch 255/300\n",
      "120/120 [==============================] - 0s 95us/sample - loss: 0.4988 - acc: 0.7750 - val_loss: 0.5518 - val_acc: 0.7000\n",
      "Epoch 256/300\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.4978 - acc: 0.7750 - val_loss: 0.5503 - val_acc: 0.7000\n",
      "Epoch 257/300\n",
      "120/120 [==============================] - 0s 136us/sample - loss: 0.4964 - acc: 0.7750 - val_loss: 0.5490 - val_acc: 0.7000\n",
      "Epoch 258/300\n",
      "120/120 [==============================] - 0s 155us/sample - loss: 0.4953 - acc: 0.7750 - val_loss: 0.5480 - val_acc: 0.7000\n",
      "Epoch 259/300\n",
      "120/120 [==============================] - 0s 144us/sample - loss: 0.4939 - acc: 0.7750 - val_loss: 0.5467 - val_acc: 0.7000\n",
      "Epoch 260/300\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.4928 - acc: 0.7750 - val_loss: 0.5453 - val_acc: 0.7000\n",
      "Epoch 261/300\n",
      "120/120 [==============================] - 0s 114us/sample - loss: 0.4916 - acc: 0.7750 - val_loss: 0.5440 - val_acc: 0.7000\n",
      "Epoch 262/300\n",
      "120/120 [==============================] - 0s 106us/sample - loss: 0.4904 - acc: 0.7833 - val_loss: 0.5427 - val_acc: 0.7000\n",
      "Epoch 263/300\n",
      "120/120 [==============================] - 0s 94us/sample - loss: 0.4892 - acc: 0.7833 - val_loss: 0.5415 - val_acc: 0.7000\n",
      "Epoch 264/300\n",
      "120/120 [==============================] - 0s 156us/sample - loss: 0.4881 - acc: 0.7833 - val_loss: 0.5402 - val_acc: 0.7000\n",
      "Epoch 265/300\n",
      "120/120 [==============================] - 0s 134us/sample - loss: 0.4869 - acc: 0.7833 - val_loss: 0.5390 - val_acc: 0.7000\n",
      "Epoch 266/300\n",
      "120/120 [==============================] - 0s 115us/sample - loss: 0.4858 - acc: 0.7833 - val_loss: 0.5379 - val_acc: 0.7000\n",
      "Epoch 267/300\n",
      "120/120 [==============================] - 0s 162us/sample - loss: 0.4847 - acc: 0.7833 - val_loss: 0.5368 - val_acc: 0.7000\n",
      "Epoch 268/300\n",
      "120/120 [==============================] - 0s 83us/sample - loss: 0.4837 - acc: 0.7833 - val_loss: 0.5354 - val_acc: 0.7000\n",
      "Epoch 269/300\n",
      "120/120 [==============================] - 0s 83us/sample - loss: 0.4824 - acc: 0.7833 - val_loss: 0.5342 - val_acc: 0.7333\n",
      "Epoch 270/300\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.4813 - acc: 0.7833 - val_loss: 0.5330 - val_acc: 0.7667\n",
      "Epoch 271/300\n",
      "120/120 [==============================] - 0s 268us/sample - loss: 0.4802 - acc: 0.7833 - val_loss: 0.5318 - val_acc: 0.7667\n",
      "Epoch 272/300\n",
      "120/120 [==============================] - 0s 317us/sample - loss: 0.4791 - acc: 0.7833 - val_loss: 0.5307 - val_acc: 0.7667\n",
      "Epoch 273/300\n",
      "120/120 [==============================] - 0s 301us/sample - loss: 0.4780 - acc: 0.7833 - val_loss: 0.5296 - val_acc: 0.7667\n",
      "Epoch 274/300\n",
      "120/120 [==============================] - 0s 219us/sample - loss: 0.4771 - acc: 0.7833 - val_loss: 0.5285 - val_acc: 0.7667\n",
      "Epoch 275/300\n",
      "120/120 [==============================] - 0s 118us/sample - loss: 0.4759 - acc: 0.7833 - val_loss: 0.5274 - val_acc: 0.7667\n",
      "Epoch 276/300\n",
      "120/120 [==============================] - 0s 131us/sample - loss: 0.4749 - acc: 0.7917 - val_loss: 0.5263 - val_acc: 0.7667\n",
      "Epoch 277/300\n",
      "120/120 [==============================] - 0s 153us/sample - loss: 0.4738 - acc: 0.7917 - val_loss: 0.5252 - val_acc: 0.7667\n",
      "Epoch 278/300\n",
      "120/120 [==============================] - 0s 249us/sample - loss: 0.4727 - acc: 0.7917 - val_loss: 0.5240 - val_acc: 0.7667\n",
      "Epoch 279/300\n",
      "120/120 [==============================] - 0s 991us/sample - loss: 0.4717 - acc: 0.7917 - val_loss: 0.5228 - val_acc: 0.8000\n",
      "Epoch 280/300\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 0.4707 - acc: 0.7917 - val_loss: 0.5215 - val_acc: 0.8000\n",
      "Epoch 281/300\n",
      "120/120 [==============================] - 0s 174us/sample - loss: 0.4696 - acc: 0.7917 - val_loss: 0.5203 - val_acc: 0.8000\n",
      "Epoch 282/300\n",
      "120/120 [==============================] - 0s 131us/sample - loss: 0.4687 - acc: 0.7917 - val_loss: 0.5191 - val_acc: 0.8000\n",
      "Epoch 283/300\n",
      "120/120 [==============================] - 0s 143us/sample - loss: 0.4677 - acc: 0.7917 - val_loss: 0.5182 - val_acc: 0.8000\n",
      "Epoch 284/300\n",
      "120/120 [==============================] - 0s 470us/sample - loss: 0.4667 - acc: 0.7917 - val_loss: 0.5170 - val_acc: 0.8000\n",
      "Epoch 285/300\n",
      "120/120 [==============================] - 0s 337us/sample - loss: 0.4657 - acc: 0.7917 - val_loss: 0.5161 - val_acc: 0.8000\n",
      "Epoch 286/300\n",
      "120/120 [==============================] - 0s 101us/sample - loss: 0.4647 - acc: 0.7917 - val_loss: 0.5150 - val_acc: 0.8000\n",
      "Epoch 287/300\n",
      "120/120 [==============================] - 0s 101us/sample - loss: 0.4637 - acc: 0.7917 - val_loss: 0.5139 - val_acc: 0.8000\n",
      "Epoch 288/300\n",
      "120/120 [==============================] - 0s 80us/sample - loss: 0.4627 - acc: 0.7917 - val_loss: 0.5130 - val_acc: 0.8000\n",
      "Epoch 289/300\n",
      "120/120 [==============================] - 0s 99us/sample - loss: 0.4618 - acc: 0.7917 - val_loss: 0.5120 - val_acc: 0.8000\n",
      "Epoch 290/300\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.4609 - acc: 0.7917 - val_loss: 0.5109 - val_acc: 0.8000\n",
      "Epoch 291/300\n",
      "120/120 [==============================] - 0s 109us/sample - loss: 0.4598 - acc: 0.7917 - val_loss: 0.5099 - val_acc: 0.8000\n",
      "Epoch 292/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.4589 - acc: 0.7917 - val_loss: 0.5090 - val_acc: 0.8000\n",
      "Epoch 293/300\n",
      "120/120 [==============================] - 0s 94us/sample - loss: 0.4580 - acc: 0.7917 - val_loss: 0.5080 - val_acc: 0.8000\n",
      "Epoch 294/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 100us/sample - loss: 0.4571 - acc: 0.7917 - val_loss: 0.5070 - val_acc: 0.8000\n",
      "Epoch 295/300\n",
      "120/120 [==============================] - 0s 96us/sample - loss: 0.4561 - acc: 0.7917 - val_loss: 0.5060 - val_acc: 0.8000\n",
      "Epoch 296/300\n",
      "120/120 [==============================] - 0s 98us/sample - loss: 0.4552 - acc: 0.8000 - val_loss: 0.5050 - val_acc: 0.8000\n",
      "Epoch 297/300\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.4543 - acc: 0.8000 - val_loss: 0.5040 - val_acc: 0.8000\n",
      "Epoch 298/300\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.4534 - acc: 0.8000 - val_loss: 0.5030 - val_acc: 0.8000\n",
      "Epoch 299/300\n",
      "120/120 [==============================] - 0s 113us/sample - loss: 0.4525 - acc: 0.8000 - val_loss: 0.5020 - val_acc: 0.8000\n",
      "Epoch 300/300\n",
      "120/120 [==============================] - 0s 99us/sample - loss: 0.4516 - acc: 0.8000 - val_loss: 0.5010 - val_acc: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a25f4ed68>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaled_X_train, \n",
    "          y=y_train, \n",
    "          epochs=300,\n",
    "          validation_data=(scaled_X_test, y_test), verbose=1 ,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.071370</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.097754</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.067721</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.093684</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.064090</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.089910</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.060764</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.086192</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.057379</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.082451</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.455223</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.505008</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.454304</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.504034</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.453408</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.503038</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.452512</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.502018</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.451639</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500988</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss       acc  val_loss   val_acc\n",
       "0    1.071370  0.333333  1.097754  0.333333\n",
       "1    1.067721  0.333333  1.093684  0.333333\n",
       "2    1.064090  0.333333  1.089910  0.333333\n",
       "3    1.060764  0.333333  1.086192  0.333333\n",
       "4    1.057379  0.333333  1.082451  0.333333\n",
       "..        ...       ...       ...       ...\n",
       "295  0.455223  0.800000  0.505008  0.800000\n",
       "296  0.454304  0.800000  0.504034  0.800000\n",
       "297  0.453408  0.800000  0.503038  0.800000\n",
       "298  0.452512  0.800000  0.502018  0.800000\n",
       "299  0.451639  0.800000  0.500988  0.800000\n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a26038cf8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a260e84a8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[['acc','val_acc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.500988245010376, 0.8]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(scaled_X_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = len(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=4,activation='relu'))\n",
    "\n",
    "# Last layer for multi-class classification of 3 species\n",
    "model.add(Dense(units=3,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "150/150 [==============================] - 0s 2ms/sample - loss: 1.1163 - acc: 0.3267\n",
      "Epoch 2/300\n",
      "150/150 [==============================] - 0s 177us/sample - loss: 1.1107 - acc: 0.3267\n",
      "Epoch 3/300\n",
      "150/150 [==============================] - 0s 209us/sample - loss: 1.1043 - acc: 0.3400\n",
      "Epoch 4/300\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 1.0985 - acc: 0.3333\n",
      "Epoch 5/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 1.0926 - acc: 0.3267\n",
      "Epoch 6/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 1.0869 - acc: 0.3267\n",
      "Epoch 7/300\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 1.0814 - acc: 0.3133\n",
      "Epoch 8/300\n",
      "150/150 [==============================] - 0s 158us/sample - loss: 1.0754 - acc: 0.3133\n",
      "Epoch 9/300\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 1.0703 - acc: 0.3267\n",
      "Epoch 10/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 1.0646 - acc: 0.3200\n",
      "Epoch 11/300\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 1.0595 - acc: 0.3400\n",
      "Epoch 12/300\n",
      "150/150 [==============================] - 0s 215us/sample - loss: 1.0540 - acc: 0.3467\n",
      "Epoch 13/300\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 1.0485 - acc: 0.3533\n",
      "Epoch 14/300\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 1.0436 - acc: 0.3733\n",
      "Epoch 15/300\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 1.0378 - acc: 0.3933\n",
      "Epoch 16/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 1.0326 - acc: 0.4200\n",
      "Epoch 17/300\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 1.0275 - acc: 0.4867\n",
      "Epoch 18/300\n",
      "150/150 [==============================] - 0s 141us/sample - loss: 1.0220 - acc: 0.5267\n",
      "Epoch 19/300\n",
      "150/150 [==============================] - 0s 150us/sample - loss: 1.0168 - acc: 0.5467\n",
      "Epoch 20/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 1.0113 - acc: 0.5467\n",
      "Epoch 21/300\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 1.0062 - acc: 0.5600\n",
      "Epoch 22/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 1.0007 - acc: 0.5733\n",
      "Epoch 23/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.9953 - acc: 0.6000\n",
      "Epoch 24/300\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.9897 - acc: 0.6200\n",
      "Epoch 25/300\n",
      "150/150 [==============================] - 0s 149us/sample - loss: 0.9842 - acc: 0.6467\n",
      "Epoch 26/300\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.9787 - acc: 0.6333\n",
      "Epoch 27/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.9733 - acc: 0.6533\n",
      "Epoch 28/300\n",
      "150/150 [==============================] - 0s 161us/sample - loss: 0.9676 - acc: 0.6467\n",
      "Epoch 29/300\n",
      "150/150 [==============================] - 0s 159us/sample - loss: 0.9619 - acc: 0.6533\n",
      "Epoch 30/300\n",
      "150/150 [==============================] - 0s 141us/sample - loss: 0.9563 - acc: 0.6533\n",
      "Epoch 31/300\n",
      "150/150 [==============================] - 0s 183us/sample - loss: 0.9509 - acc: 0.6600\n",
      "Epoch 32/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.9451 - acc: 0.6600\n",
      "Epoch 33/300\n",
      "150/150 [==============================] - 0s 172us/sample - loss: 0.9394 - acc: 0.6600\n",
      "Epoch 34/300\n",
      "150/150 [==============================] - 0s 144us/sample - loss: 0.9337 - acc: 0.6600\n",
      "Epoch 35/300\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.9281 - acc: 0.6667\n",
      "Epoch 36/300\n",
      "150/150 [==============================] - 0s 144us/sample - loss: 0.9224 - acc: 0.6667\n",
      "Epoch 37/300\n",
      "150/150 [==============================] - 0s 192us/sample - loss: 0.9170 - acc: 0.6667\n",
      "Epoch 38/300\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.9114 - acc: 0.6667\n",
      "Epoch 39/300\n",
      "150/150 [==============================] - 0s 176us/sample - loss: 0.9060 - acc: 0.6667\n",
      "Epoch 40/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.9007 - acc: 0.6667\n",
      "Epoch 41/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.8952 - acc: 0.6667\n",
      "Epoch 42/300\n",
      "150/150 [==============================] - 0s 144us/sample - loss: 0.8901 - acc: 0.6667\n",
      "Epoch 43/300\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.8849 - acc: 0.6667\n",
      "Epoch 44/300\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.8797 - acc: 0.6667\n",
      "Epoch 45/300\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.8746 - acc: 0.6667\n",
      "Epoch 46/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.8697 - acc: 0.6667\n",
      "Epoch 47/300\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.8647 - acc: 0.6667\n",
      "Epoch 48/300\n",
      "150/150 [==============================] - 0s 178us/sample - loss: 0.8596 - acc: 0.6667\n",
      "Epoch 49/300\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.8547 - acc: 0.6667\n",
      "Epoch 50/300\n",
      "150/150 [==============================] - 0s 171us/sample - loss: 0.8497 - acc: 0.6667\n",
      "Epoch 51/300\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.8450 - acc: 0.6667\n",
      "Epoch 52/300\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.8401 - acc: 0.6667\n",
      "Epoch 53/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.8354 - acc: 0.6667\n",
      "Epoch 54/300\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.8307 - acc: 0.6667\n",
      "Epoch 55/300\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.8260 - acc: 0.6667\n",
      "Epoch 56/300\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.8214 - acc: 0.6667\n",
      "Epoch 57/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.8166 - acc: 0.6667\n",
      "Epoch 58/300\n",
      "150/150 [==============================] - 0s 143us/sample - loss: 0.8121 - acc: 0.6667\n",
      "Epoch 59/300\n",
      "150/150 [==============================] - 0s 167us/sample - loss: 0.8074 - acc: 0.6667\n",
      "Epoch 60/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.8029 - acc: 0.6667\n",
      "Epoch 61/300\n",
      "150/150 [==============================] - 0s 176us/sample - loss: 0.7984 - acc: 0.6667\n",
      "Epoch 62/300\n",
      "150/150 [==============================] - 0s 146us/sample - loss: 0.7938 - acc: 0.6667\n",
      "Epoch 63/300\n",
      "150/150 [==============================] - 0s 123us/sample - loss: 0.7892 - acc: 0.6667\n",
      "Epoch 64/300\n",
      "150/150 [==============================] - 0s 123us/sample - loss: 0.7847 - acc: 0.6667\n",
      "Epoch 65/300\n",
      "150/150 [==============================] - 0s 166us/sample - loss: 0.7803 - acc: 0.6667\n",
      "Epoch 66/300\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.7757 - acc: 0.6667\n",
      "Epoch 67/300\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.7712 - acc: 0.6667\n",
      "Epoch 68/300\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.7668 - acc: 0.6667\n",
      "Epoch 69/300\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.7624 - acc: 0.6667\n",
      "Epoch 70/300\n",
      "150/150 [==============================] - 0s 152us/sample - loss: 0.7580 - acc: 0.6667\n",
      "Epoch 71/300\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.7535 - acc: 0.6667\n",
      "Epoch 72/300\n",
      "150/150 [==============================] - 0s 187us/sample - loss: 0.7491 - acc: 0.6667\n",
      "Epoch 73/300\n",
      "150/150 [==============================] - 0s 156us/sample - loss: 0.7449 - acc: 0.6667\n",
      "Epoch 74/300\n",
      "150/150 [==============================] - 0s 149us/sample - loss: 0.7403 - acc: 0.6667\n",
      "Epoch 75/300\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.7362 - acc: 0.6667\n",
      "Epoch 76/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.7318 - acc: 0.6667\n",
      "Epoch 77/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.7275 - acc: 0.6667\n",
      "Epoch 78/300\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 0.7233 - acc: 0.6667\n",
      "Epoch 79/300\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.7190 - acc: 0.6667\n",
      "Epoch 80/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.7150 - acc: 0.6667\n",
      "Epoch 81/300\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.7107 - acc: 0.6667\n",
      "Epoch 82/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.7066 - acc: 0.6667\n",
      "Epoch 83/300\n",
      "150/150 [==============================] - 0s 145us/sample - loss: 0.7024 - acc: 0.6667\n",
      "Epoch 84/300\n",
      "150/150 [==============================] - 0s 155us/sample - loss: 0.6983 - acc: 0.6667\n",
      "Epoch 85/300\n",
      "150/150 [==============================] - 0s 148us/sample - loss: 0.6942 - acc: 0.6667\n",
      "Epoch 86/300\n",
      "150/150 [==============================] - 0s 144us/sample - loss: 0.6901 - acc: 0.6667\n",
      "Epoch 87/300\n",
      "150/150 [==============================] - 0s 141us/sample - loss: 0.6860 - acc: 0.6667\n",
      "Epoch 88/300\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.6820 - acc: 0.6667\n",
      "Epoch 89/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.6780 - acc: 0.6667\n",
      "Epoch 90/300\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 0.6739 - acc: 0.6667\n",
      "Epoch 91/300\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.6699 - acc: 0.6667\n",
      "Epoch 92/300\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.6658 - acc: 0.6667\n",
      "Epoch 93/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.6618 - acc: 0.6667\n",
      "Epoch 94/300\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.6579 - acc: 0.6667\n",
      "Epoch 95/300\n",
      "150/150 [==============================] - 0s 150us/sample - loss: 0.6539 - acc: 0.6667\n",
      "Epoch 96/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.6499 - acc: 0.6667\n",
      "Epoch 97/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.6459 - acc: 0.6667\n",
      "Epoch 98/300\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.6420 - acc: 0.6667\n",
      "Epoch 99/300\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.6379 - acc: 0.6733\n",
      "Epoch 100/300\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.6340 - acc: 0.6733\n",
      "Epoch 101/300\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.6299 - acc: 0.6733\n",
      "Epoch 102/300\n",
      "150/150 [==============================] - 0s 138us/sample - loss: 0.6260 - acc: 0.6733\n",
      "Epoch 103/300\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.6221 - acc: 0.6733\n",
      "Epoch 104/300\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.6181 - acc: 0.6733\n",
      "Epoch 105/300\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.6142 - acc: 0.6733\n",
      "Epoch 106/300\n",
      "150/150 [==============================] - 0s 148us/sample - loss: 0.6104 - acc: 0.6867\n",
      "Epoch 107/300\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.6064 - acc: 0.6933\n",
      "Epoch 108/300\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.6024 - acc: 0.6933\n",
      "Epoch 109/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.5986 - acc: 0.7000\n",
      "Epoch 110/300\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.5947 - acc: 0.7067\n",
      "Epoch 111/300\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.5909 - acc: 0.7133\n",
      "Epoch 112/300\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.5872 - acc: 0.7133\n",
      "Epoch 113/300\n",
      "150/150 [==============================] - 0s 132us/sample - loss: 0.5836 - acc: 0.7133\n",
      "Epoch 114/300\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.5803 - acc: 0.7133\n",
      "Epoch 115/300\n",
      "150/150 [==============================] - 0s 74us/sample - loss: 0.5767 - acc: 0.7133\n",
      "Epoch 116/300\n",
      "150/150 [==============================] - 0s 155us/sample - loss: 0.5733 - acc: 0.7133\n",
      "Epoch 117/300\n",
      "150/150 [==============================] - 0s 259us/sample - loss: 0.5700 - acc: 0.7133\n",
      "Epoch 118/300\n",
      "150/150 [==============================] - 0s 155us/sample - loss: 0.5666 - acc: 0.7133\n",
      "Epoch 119/300\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.5634 - acc: 0.7133\n",
      "Epoch 120/300\n",
      "150/150 [==============================] - 0s 179us/sample - loss: 0.5602 - acc: 0.7200\n",
      "Epoch 121/300\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.5569 - acc: 0.7267\n",
      "Epoch 122/300\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.5538 - acc: 0.7267\n",
      "Epoch 123/300\n",
      "150/150 [==============================] - 0s 191us/sample - loss: 0.5507 - acc: 0.7333\n",
      "Epoch 124/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.5476 - acc: 0.7333\n",
      "Epoch 125/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.5446 - acc: 0.7400\n",
      "Epoch 126/300\n",
      "150/150 [==============================] - 0s 183us/sample - loss: 0.5416 - acc: 0.7467\n",
      "Epoch 127/300\n",
      "150/150 [==============================] - 0s 139us/sample - loss: 0.5386 - acc: 0.7467\n",
      "Epoch 128/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.5357 - acc: 0.7467\n",
      "Epoch 129/300\n",
      "150/150 [==============================] - 0s 143us/sample - loss: 0.5329 - acc: 0.7533\n",
      "Epoch 130/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.5301 - acc: 0.7533\n",
      "Epoch 131/300\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.5273 - acc: 0.7733\n",
      "Epoch 132/300\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.5246 - acc: 0.7733\n",
      "Epoch 133/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.5219 - acc: 0.7733\n",
      "Epoch 134/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.5192 - acc: 0.7867\n",
      "Epoch 135/300\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.5166 - acc: 0.7867\n",
      "Epoch 136/300\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.5140 - acc: 0.7933\n",
      "Epoch 137/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.5257 - acc: 0.875 - 0s 84us/sample - loss: 0.5115 - acc: 0.8133\n",
      "Epoch 138/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.5090 - acc: 0.8400\n",
      "Epoch 139/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.5065 - acc: 0.8667\n",
      "Epoch 140/300\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.5040 - acc: 0.8800\n",
      "Epoch 141/300\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.5016 - acc: 0.8800\n",
      "Epoch 142/300\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.4992 - acc: 0.8800\n",
      "Epoch 143/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.4969 - acc: 0.8800\n",
      "Epoch 144/300\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.4945 - acc: 0.8800\n",
      "Epoch 145/300\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.4922 - acc: 0.8867\n",
      "Epoch 146/300\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.4900 - acc: 0.8933\n",
      "Epoch 147/300\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.4877 - acc: 0.8933\n",
      "Epoch 148/300\n",
      "150/150 [==============================] - 0s 151us/sample - loss: 0.4854 - acc: 0.8933\n",
      "Epoch 149/300\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.4833 - acc: 0.8933\n",
      "Epoch 150/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.4811 - acc: 0.8933\n",
      "Epoch 151/300\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.4790 - acc: 0.9000\n",
      "Epoch 152/300\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.4769 - acc: 0.9000\n",
      "Epoch 153/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.4747 - acc: 0.9067\n",
      "Epoch 154/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.4727 - acc: 0.9067\n",
      "Epoch 155/300\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.4707 - acc: 0.9067\n",
      "Epoch 156/300\n",
      "150/150 [==============================] - 0s 174us/sample - loss: 0.4687 - acc: 0.9067\n",
      "Epoch 157/300\n",
      "150/150 [==============================] - 0s 192us/sample - loss: 0.4666 - acc: 0.9067\n",
      "Epoch 158/300\n",
      "150/150 [==============================] - 0s 168us/sample - loss: 0.4648 - acc: 0.9067\n",
      "Epoch 159/300\n",
      "150/150 [==============================] - 0s 142us/sample - loss: 0.4628 - acc: 0.9067\n",
      "Epoch 160/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4608 - acc: 0.9067\n",
      "Epoch 161/300\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.4589 - acc: 0.9133\n",
      "Epoch 162/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 212us/sample - loss: 0.4570 - acc: 0.9133\n",
      "Epoch 163/300\n",
      "150/150 [==============================] - 0s 149us/sample - loss: 0.4552 - acc: 0.9200\n",
      "Epoch 164/300\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.4533 - acc: 0.9200\n",
      "Epoch 165/300\n",
      "150/150 [==============================] - 0s 132us/sample - loss: 0.4514 - acc: 0.9200\n",
      "Epoch 166/300\n",
      "150/150 [==============================] - 0s 152us/sample - loss: 0.4496 - acc: 0.9267\n",
      "Epoch 167/300\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.4478 - acc: 0.9267\n",
      "Epoch 168/300\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.4460 - acc: 0.9333\n",
      "Epoch 169/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.4443 - acc: 0.9333\n",
      "Epoch 170/300\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.4425 - acc: 0.9333\n",
      "Epoch 171/300\n",
      "150/150 [==============================] - 0s 148us/sample - loss: 0.4408 - acc: 0.9333\n",
      "Epoch 172/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.4391 - acc: 0.9333\n",
      "Epoch 173/300\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.4374 - acc: 0.9333\n",
      "Epoch 174/300\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.4357 - acc: 0.9333\n",
      "Epoch 175/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.4341 - acc: 0.9333\n",
      "Epoch 176/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.4324 - acc: 0.9333\n",
      "Epoch 177/300\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.4307 - acc: 0.9333\n",
      "Epoch 178/300\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.4290 - acc: 0.9333\n",
      "Epoch 179/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.4275 - acc: 0.9333\n",
      "Epoch 180/300\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.4259 - acc: 0.9333\n",
      "Epoch 181/300\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.4242 - acc: 0.9333\n",
      "Epoch 182/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.4228 - acc: 0.9333\n",
      "Epoch 183/300\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.4211 - acc: 0.9333\n",
      "Epoch 184/300\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 0.4195 - acc: 0.9400\n",
      "Epoch 185/300\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.4181 - acc: 0.9400\n",
      "Epoch 186/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4164 - acc: 0.9400\n",
      "Epoch 187/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.4150 - acc: 0.9467\n",
      "Epoch 188/300\n",
      "150/150 [==============================] - 0s 154us/sample - loss: 0.4134 - acc: 0.9467\n",
      "Epoch 189/300\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.4118 - acc: 0.9467\n",
      "Epoch 190/300\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.4104 - acc: 0.9467\n",
      "Epoch 191/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.4089 - acc: 0.9467\n",
      "Epoch 192/300\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.4074 - acc: 0.9467\n",
      "Epoch 193/300\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 0.4059 - acc: 0.9467\n",
      "Epoch 194/300\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.4046 - acc: 0.9400\n",
      "Epoch 195/300\n",
      "150/150 [==============================] - 0s 150us/sample - loss: 0.4030 - acc: 0.9400\n",
      "Epoch 196/300\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.4015 - acc: 0.9467\n",
      "Epoch 197/300\n",
      "150/150 [==============================] - 0s 148us/sample - loss: 0.4002 - acc: 0.9467\n",
      "Epoch 198/300\n",
      "150/150 [==============================] - 0s 157us/sample - loss: 0.3987 - acc: 0.9467\n",
      "Epoch 199/300\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.3973 - acc: 0.9467\n",
      "Epoch 200/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.3959 - acc: 0.9467\n",
      "Epoch 201/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3945 - acc: 0.9467\n",
      "Epoch 202/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3931 - acc: 0.9467\n",
      "Epoch 203/300\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3917 - acc: 0.9467\n",
      "Epoch 204/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.3904 - acc: 0.9533\n",
      "Epoch 205/300\n",
      "150/150 [==============================] - 0s 75us/sample - loss: 0.3890 - acc: 0.9533\n",
      "Epoch 206/300\n",
      "150/150 [==============================] - 0s 225us/sample - loss: 0.3876 - acc: 0.9533\n",
      "Epoch 207/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3862 - acc: 0.9533\n",
      "Epoch 208/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.3849 - acc: 0.9533\n",
      "Epoch 209/300\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.3836 - acc: 0.9533\n",
      "Epoch 210/300\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.3822 - acc: 0.9533\n",
      "Epoch 211/300\n",
      "150/150 [==============================] - 0s 75us/sample - loss: 0.3810 - acc: 0.9533\n",
      "Epoch 212/300\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.3796 - acc: 0.9533\n",
      "Epoch 213/300\n",
      "150/150 [==============================] - 0s 159us/sample - loss: 0.3783 - acc: 0.9533\n",
      "Epoch 214/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3770 - acc: 0.9533\n",
      "Epoch 215/300\n",
      "150/150 [==============================] - 0s 168us/sample - loss: 0.3757 - acc: 0.9533\n",
      "Epoch 216/300\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.3747 - acc: 0.9533\n",
      "Epoch 217/300\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.3732 - acc: 0.9533\n",
      "Epoch 218/300\n",
      "150/150 [==============================] - 0s 69us/sample - loss: 0.3719 - acc: 0.9533\n",
      "Epoch 219/300\n",
      "150/150 [==============================] - 0s 158us/sample - loss: 0.3706 - acc: 0.9533\n",
      "Epoch 220/300\n",
      "150/150 [==============================] - 0s 156us/sample - loss: 0.3693 - acc: 0.9533\n",
      "Epoch 221/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.3682 - acc: 0.9533\n",
      "Epoch 222/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3669 - acc: 0.9533\n",
      "Epoch 223/300\n",
      "150/150 [==============================] - 0s 156us/sample - loss: 0.3656 - acc: 0.9533\n",
      "Epoch 224/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3644 - acc: 0.9533\n",
      "Epoch 225/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.3631 - acc: 0.9533\n",
      "Epoch 226/300\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.3619 - acc: 0.9533\n",
      "Epoch 227/300\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.3607 - acc: 0.9533\n",
      "Epoch 228/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3595 - acc: 0.9533\n",
      "Epoch 229/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.3583 - acc: 0.9533\n",
      "Epoch 230/300\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.3571 - acc: 0.9600\n",
      "Epoch 231/300\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.3559 - acc: 0.9600\n",
      "Epoch 232/300\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.3548 - acc: 0.9600\n",
      "Epoch 233/300\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.3536 - acc: 0.9600\n",
      "Epoch 234/300\n",
      "150/150 [==============================] - 0s 156us/sample - loss: 0.3524 - acc: 0.9600\n",
      "Epoch 235/300\n",
      "150/150 [==============================] - 0s 138us/sample - loss: 0.3512 - acc: 0.9600\n",
      "Epoch 236/300\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.3500 - acc: 0.9600\n",
      "Epoch 237/300\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.3489 - acc: 0.9600\n",
      "Epoch 238/300\n",
      "150/150 [==============================] - 0s 164us/sample - loss: 0.3477 - acc: 0.9600\n",
      "Epoch 239/300\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.3466 - acc: 0.9600\n",
      "Epoch 240/300\n",
      "150/150 [==============================] - 0s 132us/sample - loss: 0.3454 - acc: 0.9600\n",
      "Epoch 241/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.3443 - acc: 0.9600\n",
      "Epoch 242/300\n",
      "150/150 [==============================] - 0s 143us/sample - loss: 0.3431 - acc: 0.9600\n",
      "Epoch 243/300\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.3420 - acc: 0.9600\n",
      "Epoch 244/300\n",
      "150/150 [==============================] - 0s 132us/sample - loss: 0.3409 - acc: 0.9600\n",
      "Epoch 245/300\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.3398 - acc: 0.9600\n",
      "Epoch 246/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.3386 - acc: 0.9667\n",
      "Epoch 247/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.3376 - acc: 0.9667\n",
      "Epoch 248/300\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.3364 - acc: 0.9667\n",
      "Epoch 249/300\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.3353 - acc: 0.9667\n",
      "Epoch 250/300\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.3342 - acc: 0.9667\n",
      "Epoch 251/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.3331 - acc: 0.9667\n",
      "Epoch 252/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.3320 - acc: 0.9667\n",
      "Epoch 253/300\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.3311 - acc: 0.9667\n",
      "Epoch 254/300\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.3298 - acc: 0.9667\n",
      "Epoch 255/300\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3288 - acc: 0.9667\n",
      "Epoch 256/300\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.3276 - acc: 0.9667\n",
      "Epoch 257/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.3265 - acc: 0.9667\n",
      "Epoch 258/300\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.3255 - acc: 0.9667\n",
      "Epoch 259/300\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.3247 - acc: 0.9667\n",
      "Epoch 260/300\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.3236 - acc: 0.9667\n",
      "Epoch 261/300\n",
      "150/150 [==============================] - 0s 139us/sample - loss: 0.3223 - acc: 0.9667\n",
      "Epoch 262/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3213 - acc: 0.9600\n",
      "Epoch 263/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.3203 - acc: 0.9600\n",
      "Epoch 264/300\n",
      "150/150 [==============================] - 0s 71us/sample - loss: 0.3193 - acc: 0.9600\n",
      "Epoch 265/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.3182 - acc: 0.9600\n",
      "Epoch 266/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.3171 - acc: 0.9667\n",
      "Epoch 267/300\n",
      "150/150 [==============================] - 0s 148us/sample - loss: 0.3161 - acc: 0.9667\n",
      "Epoch 268/300\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.3152 - acc: 0.9667\n",
      "Epoch 269/300\n",
      "150/150 [==============================] - 0s 148us/sample - loss: 0.3142 - acc: 0.9667\n",
      "Epoch 270/300\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.3131 - acc: 0.9667\n",
      "Epoch 271/300\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3121 - acc: 0.9667\n",
      "Epoch 272/300\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.3111 - acc: 0.9667\n",
      "Epoch 273/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.3101 - acc: 0.9667\n",
      "Epoch 274/300\n",
      "150/150 [==============================] - 0s 64us/sample - loss: 0.3091 - acc: 0.9600\n",
      "Epoch 275/300\n",
      "150/150 [==============================] - 0s 68us/sample - loss: 0.3081 - acc: 0.9600\n",
      "Epoch 276/300\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.3073 - acc: 0.9600\n",
      "Epoch 277/300\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 0.3061 - acc: 0.9600\n",
      "Epoch 278/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3051 - acc: 0.9600\n",
      "Epoch 279/300\n",
      "150/150 [==============================] - 0s 156us/sample - loss: 0.3041 - acc: 0.9600\n",
      "Epoch 280/300\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.3032 - acc: 0.9600\n",
      "Epoch 281/300\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.3022 - acc: 0.9600\n",
      "Epoch 282/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.3014 - acc: 0.9600\n",
      "Epoch 283/300\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.3005 - acc: 0.9600\n",
      "Epoch 284/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.2994 - acc: 0.9600\n",
      "Epoch 285/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2984 - acc: 0.9600\n",
      "Epoch 286/300\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.2975 - acc: 0.9600\n",
      "Epoch 287/300\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.2967 - acc: 0.9600\n",
      "Epoch 288/300\n",
      "150/150 [==============================] - 0s 169us/sample - loss: 0.2956 - acc: 0.9600\n",
      "Epoch 289/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.2947 - acc: 0.9600\n",
      "Epoch 290/300\n",
      "150/150 [==============================] - 0s 156us/sample - loss: 0.2937 - acc: 0.9600\n",
      "Epoch 291/300\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.2928 - acc: 0.9600\n",
      "Epoch 292/300\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.2919 - acc: 0.9600\n",
      "Epoch 293/300\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.2909 - acc: 0.9600\n",
      "Epoch 294/300\n",
      "150/150 [==============================] - 0s 168us/sample - loss: 0.2900 - acc: 0.9600\n",
      "Epoch 295/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.2892 - acc: 0.9600\n",
      "Epoch 296/300\n",
      "150/150 [==============================] - 0s 154us/sample - loss: 0.2882 - acc: 0.9600\n",
      "Epoch 297/300\n",
      "150/150 [==============================] - 0s 184us/sample - loss: 0.2873 - acc: 0.9600\n",
      "Epoch 298/300\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.2864 - acc: 0.9600\n",
      "Epoch 299/300\n",
      "150/150 [==============================] - 0s 177us/sample - loss: 0.2855 - acc: 0.9600\n",
      "Epoch 300/300\n",
      "150/150 [==============================] - 0s 168us/sample - loss: 0.2845 - acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a26a26390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_X,y,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"iris_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Scaler\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_scaler.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler,'iris_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a Single New Flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "flower_model = load_model(\"iris_model.h5\")\n",
    "flower_scaler = joblib.load(\"iris_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_example = {'sepal_length':5.1,\n",
    "                 'sepal_width':3.5,\n",
    "                 'petal_length':1.4,\n",
    "                 'petal_width':0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flower_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prediction(model,scaler,sample_json):\n",
    "    \n",
    "    s_len = sample_json['sepal_length']\n",
    "    s_wid = sample_json['sepal_width']\n",
    "    p_len = sample_json['petal_length']\n",
    "    p_wid = sample_json['petal_width']\n",
    "    \n",
    "    flower = [[s_len,s_wid,p_len,p_wid]]\n",
    "    \n",
    "    flower = scaler.transform(flower)\n",
    "    \n",
    "    classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    \n",
    "    class_ind = model.predict_classes(flower)\n",
    "    \n",
    "    return classes[class_ind][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_prediction(flower_model,flower_scaler,flower_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "\n",
    "flower_model = load_model(\"iris_model.h5\")\n",
    "flower_scaler = joblib.load(\"iris_scaler.pkl\")\n",
    "\n",
    "\n",
    "def return_prediction(model,scaler,sample_json):\n",
    "    \n",
    "    s_len = sample_json['sepal_length']\n",
    "    s_wid = sample_json['sepal_width']\n",
    "    p_len = sample_json['petal_length']\n",
    "    p_wid = sample_json['petal_width']\n",
    "    \n",
    "    flower = [[s_len,s_wid,p_len,p_wid]]\n",
    "    \n",
    "    flower = scaler.transform(flower)\n",
    "    \n",
    "    classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    \n",
    "    class_ind = model.predict_classes(flower)\n",
    "    \n",
    "    return classes[class_ind][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
